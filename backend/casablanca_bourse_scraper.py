"""
Script de Web Scraping - Bourse de Casablanca
==============================================
Ce script extrait toutes les donnÃ©es des actions cotÃ©es Ã  la Bourse de Casablanca.

Auteur: SystÃ¨me de Trading TradeOrange
Date: 2026-01-15
"""

import requests
from bs4 import BeautifulSoup
import json
import csv
import time
from datetime import datetime
from typing import List, Dict
import logging

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class CasablancaBourseScaper:
    """
    Classe pour scraper les donnÃ©es de la Bourse de Casablanca
    """
    
    def __init__(self):
        """Initialisation du scraper"""
        self.base_url = "https://www.casablanca-bourse.com"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'fr-FR,fr;q=0.9,en;q=0.8',
            'Connection': 'keep-alive',
        })
        self.stocks_data = []
    
    def fetch_page(self, url: str, max_retries: int = 3) -> requests.Response:
        """
        RÃ©cupÃ¨re une page avec gestion des erreurs et retry
        
        Args:
            url: URL Ã  rÃ©cupÃ©rer
            max_retries: Nombre maximum de tentatives
            
        Returns:
            Response object
        """
        for attempt in range(max_retries):
            try:
                logger.info(f"Tentative {attempt + 1}/{max_retries} pour {url}")
                response = self.session.get(url, timeout=30)
                response.raise_for_status()
                
                # DÃ©lai entre les requÃªtes pour Ã©viter le blocage
                time.sleep(2)
                return response
                
            except requests.RequestException as e:
                logger.warning(f"Erreur lors de la requÃªte: {e}")
                if attempt == max_retries - 1:
                    logger.error(f"Ã‰chec aprÃ¨s {max_retries} tentatives")
                    raise
                time.sleep(5)
    
    def check_for_api(self) -> bool:
        """
        VÃ©rifie s'il existe une API interne pour les cotations
        
        Returns:
            True si API dÃ©tectÃ©e, False sinon
        """
        logger.info("VÃ©rification de l'existence d'une API interne...")
        
        # URLs API potentielles
        api_urls = [
            f"{self.base_url}/api/data/products",
            f"{self.base_url}/api/cotations",
            f"{self.base_url}/api/stocks",
            f"{self.base_url}/fr/api/cours",
        ]
        
        for api_url in api_urls:
            try:
                response = self.session.get(api_url, timeout=10)
                if response.status_code == 200 and 'application/json' in response.headers.get('Content-Type', ''):
                    logger.info(f"âœ… API trouvÃ©e: {api_url}")
                    return True
            except:
                continue
        
        logger.info("âŒ Pas d'API dÃ©tectÃ©e, utilisation du scraping HTML")
        return False
    
    def scrape_stocks_from_api(self, api_url: str) -> List[Dict]:
        """
        Extrait les donnÃ©es via API si disponible
        
        Args:
            api_url: URL de l'API
            
        Returns:
            Liste des actions avec leurs donnÃ©es
        """
        logger.info("Extraction des donnÃ©es via API...")
        
        try:
            response = self.fetch_page(api_url)
            data = response.json()
            
            stocks = []
            # Adapter selon la structure de l'API
            for item in data.get('stocks', data.get('data', [])):
                stock = {
                    'nom': item.get('name', item.get('nom', 'N/A')),
                    'isin': item.get('isin', 'N/A'),
                    'symbole': item.get('symbol', item.get('ticker', 'N/A')),
                    'dernier_cours': item.get('last', item.get('cours', 0)),
                    'variation_pct': item.get('variation', item.get('var_pct', 0)),
                    'volume': item.get('volume', 0),
                    'capitalisation': item.get('capitalization', item.get('cap', 0)),
                    'date_extraction': datetime.now().isoformat()
                }
                stocks.append(stock)
            
            logger.info(f"âœ… {len(stocks)} actions extraites via API")
            return stocks
            
        except Exception as e:
            logger.error(f"Erreur lors de l'extraction API: {e}")
            return []
    
    def scrape_stocks_from_html(self) -> List[Dict]:
        """
        Extrait les donnÃ©es par scraping HTML
        
        Returns:
            Liste des actions avec leurs donnÃ©es
        """
        logger.info("Extraction des donnÃ©es par scraping HTML...")
        
        # URL de la page des cotations
        cotations_url = f"{self.base_url}/fr/cotations/actions"
        
        try:
            response = self.fetch_page(cotations_url)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            stocks = []
            
            # Recherche des tableaux de cotations
            tables = soup.find_all('table', {'class': ['table', 'quotation', 'cours']})
            
            if not tables:
                # Fallback: chercher tous les tableaux
                tables = soup.find_all('table')
                logger.warning(f"Recherche gÃ©nÃ©rique: {len(tables)} tableaux trouvÃ©s")
            
            for table in tables:
                rows = table.find_all('tr')[1:]  # Skip header
                
                for row in rows:
                    cols = row.find_all(['td', 'th'])
                    
                    if len(cols) >= 4:  # Au minimum nom, cours, variation, volume
                        try:
                            stock = {
                                'nom': cols[0].text.strip(),
                                'symbole': cols[1].text.strip() if len(cols) > 1 else 'N/A',
                                'dernier_cours': self._parse_number(cols[2].text if len(cols) > 2 else '0'),
                                'variation_pct': self._parse_number(cols[3].text if len(cols) > 3 else '0'),
                                'volume': self._parse_number(cols[4].text if len(cols) > 4 else '0'),
                                'capitalisation': self._parse_number(cols[5].text if len(cols) > 5 else '0'),
                                'isin': 'N/A',  # Ã€ complÃ©ter si disponible
                                'date_extraction': datetime.now().isoformat()
                            }
                            
                            # VÃ©rifier que le nom n'est pas vide
                            if stock['nom'] and stock['nom'] != 'N/A':
                                stocks.append(stock)
                                
                        except Exception as e:
                            logger.warning(f"Erreur lors du parsing d'une ligne: {e}")
                            continue
            
            logger.info(f"âœ… {len(stocks)} actions extraites par scraping HTML")
            return stocks
            
        except Exception as e:
            logger.error(f"Erreur lors du scraping HTML: {e}")
            return []
    
    def scrape_individual_stock_detail(self, stock_url: str) -> Dict:
        """
        Extrait les dÃ©tails d'une action spÃ©cifique
        
        Args:
            stock_url: URL de la page de l'action
            
        Returns:
            Dictionnaire avec les dÃ©tails
        """
        try:
            response = self.fetch_page(stock_url)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            details = {}
            
            # Rechercher les informations spÃ©cifiques
            # Code ISIN
            isin_elem = soup.find(text=lambda t: t and 'ISIN' in t)
            if isin_elem:
                details['isin'] = isin_elem.find_next().text.strip()
            
            # Capitalisation
            cap_elem = soup.find(text=lambda t: t and 'Capitalisation' in t)
            if cap_elem:
                details['capitalisation'] = self._parse_number(cap_elem.find_next().text)
            
            return details
            
        except Exception as e:
            logger.warning(f"Erreur lors de l'extraction des dÃ©tails: {e}")
            return {}
    
    def _parse_number(self, text: str) -> float:
        """
        Parse un nombre depuis une chaÃ®ne (gÃ¨re les formats franÃ§ais)
        
        Args:
            text: Texte Ã  parser
            
        Returns:
            Nombre en float
        """
        try:
            # Nettoyer le texte
            text = text.strip().replace(' ', '').replace('\xa0', '')
            text = text.replace(',', '.')  # Format franÃ§ais
            text = ''.join(c for c in text if c.isdigit() or c in ['.', '-'])
            
            return float(text) if text else 0.0
        except:
            return 0.0
    
    def run(self) -> List[Dict]:
        """
        ExÃ©cute le scraping complet
        
        Returns:
            Liste des actions extraites
        """
        logger.info("ğŸš€ DÃ©marrage du scraping de la Bourse de Casablanca...")
        
        # VÃ©rifier s'il y a une API
        if self.check_for_api():
            # Tenter l'extraction via API
            self.stocks_data = self.scrape_stocks_from_api(f"{self.base_url}/api/data/products")
        
        # Si pas d'API ou Ã©chec, utiliser le scraping HTML
        if not self.stocks_data:
            self.stocks_data = self.scrape_stocks_from_html()
        
        if not self.stocks_data:
            logger.error("âŒ Aucune donnÃ©e n'a pu Ãªtre extraite!")
        else:
            logger.info(f"âœ… Extraction terminÃ©e: {len(self.stocks_data)} actions")
        
        return self.stocks_data
    
    def export_to_csv(self, filename: str = 'bourse_casablanca.csv'):
        """
        Exporte les donnÃ©es en CSV
        
        Args:
            filename: Nom du fichier CSV
        """
        if not self.stocks_data:
            logger.warning("Aucune donnÃ©e Ã  exporter")
            return
        
        logger.info(f"Export vers CSV: {filename}")
        
        try:
            with open(filename, 'w', newline='', encoding='utf-8') as f:
                if self.stocks_data:
                    writer = csv.DictWriter(f, fieldnames=self.stocks_data[0].keys())
                    writer.writeheader()
                    writer.writerows(self.stocks_data)
            
            logger.info(f"âœ… Export CSV rÃ©ussi: {filename}")
            
        except Exception as e:
            logger.error(f"âŒ Erreur lors de l'export CSV: {e}")
    
    def export_to_json(self, filename: str = 'bourse_casablanca.json'):
        """
        Exporte les donnÃ©es en JSON
        
        Args:
            filename: Nom du fichier JSON
        """
        if not self.stocks_data:
            logger.warning("Aucune donnÃ©e Ã  exporter")
            return
        
        logger.info(f"Export vers JSON: {filename}")
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump({
                    'metadata': {
                        'source': 'Bourse de Casablanca',
                        'url': self.base_url,
                        'date_extraction': datetime.now().isoformat(),
                        'nombre_actions': len(self.stocks_data)
                    },
                    'actions': self.stocks_data
                }, f, indent=2, ensure_ascii=False)
            
            logger.info(f"âœ… Export JSON rÃ©ussi: {filename}")
            
        except Exception as e:
            logger.error(f"âŒ Erreur lors de l'export JSON: {e}")
    
    def print_summary(self):
        """Affiche un rÃ©sumÃ© des donnÃ©es extraites"""
        if not self.stocks_data:
            print("\nâŒ Aucune donnÃ©e extraite")
            return
        
        print("\n" + "="*60)
        print(f"ğŸ“Š RÃ‰SUMÃ‰ - Bourse de Casablanca")
        print("="*60)
        print(f"Nombre d'actions extraites: {len(self.stocks_data)}")
        print(f"Date d'extraction: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("\nğŸ“ˆ AperÃ§u des premiÃ¨res actions:\n")
        
        for i, stock in enumerate(self.stocks_data[:5], 1):
            print(f"{i}. {stock['nom']} ({stock['symbole']})")
            print(f"   Cours: {stock['dernier_cours']} | Variation: {stock['variation_pct']}%")
            print(f"   Volume: {stock['volume']:,}")
            print()
        
        if len(self.stocks_data) > 5:
            print(f"... et {len(self.stocks_data) - 5} autres actions")
        
        print("="*60 + "\n")


def main():
    """
    Fonction principale pour exÃ©cuter le scraper
    """
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘     SCRAPER - BOURSE DE CASABLANCA                         â•‘
    â•‘     Extraction complÃ¨te des cotations                      â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    # CrÃ©er l'instance du scraper
    scraper = CasablancaBourseScaper()
    
    try:
        # ExÃ©cuter le scraping
        stocks = scraper.run()
        
        # Afficher le rÃ©sumÃ©
        scraper.print_summary()
        
        # Exporter les donnÃ©es
        if stocks:
            scraper.export_to_csv('bourse_casablanca.csv')
            scraper.export_to_json('bourse_casablanca.json')
            
            print("\nâœ… Scraping terminÃ© avec succÃ¨s!")
            print(f"ğŸ“ Fichiers gÃ©nÃ©rÃ©s:")
            print(f"   - bourse_casablanca.csv")
            print(f"   - bourse_casablanca.json")
        else:
            print("\nâš ï¸  Aucune donnÃ©e n'a Ã©tÃ© extraite")
            print("VÃ©rifiez la connexion et la structure du site")
        
    except Exception as e:
        logger.error(f"âŒ Erreur critique: {e}")
        print(f"\nâŒ Le scraping a Ã©chouÃ©: {e}")
        raise


if __name__ == "__main__":
    main()
